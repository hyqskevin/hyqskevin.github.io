<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Kevin W build this air-castle to collect infos &amp; photos"><title>scikit-learn 文档学习笔记(1) | MonoShow</title><link rel="stylesheet" type="text/css" href="../../../../css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask sizes="any" href="../../../../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../../../../favicon.ico"><link rel="apple-touch-icon" href="../../../../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../../../../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../../../../atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">scikit-learn 文档学习笔记(1)</h1><a id="logo" href="../../../../.">MonoShow</a><p class="description">Monologue from Kevin_W</p></div><div id="nav-menu"><a class="current" href="../../../../."><i class="fa fa-home"> 时间线</i></a><a href="../../../../archives/"><i class="fa fa-archive"> 档案袋</i></a><a href="../../../../categories/language-learning/"><i class="fa fa-book"> 看原著了嘛</i></a><a href="../../../../categories/github-repo/"><i class="fa fa-github"> 码代码了嘛</i></a><a href="../../../../categories/paper/"><i class="fa fa-file-text"> 翻译论文了嘛</i></a><a href="../../../../photo/"><i class="fa fa-camera"> Ins摄影集</i></a><a href="../../../../about/"><i class="fa fa-user"> 自己的碎碎念</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">scikit-learn 文档学习笔记(1)</h1><div class="post-meta">Jul 30, 2019<span> | </span><span class="category"><a href="../../../../categories/repo/">repo</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 11</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#监督学习"><span class="toc-number">1.</span> <span class="toc-text">监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性回归模型"><span class="toc-number">1.1.</span> <span class="toc-text">线性回归模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#logistic-线性分类模型"><span class="toc-number">1.2.</span> <span class="toc-text">logistic 线性分类模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD-随机梯度下降"><span class="toc-number">1.3.</span> <span class="toc-text">SGD 随机梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最近邻"><span class="toc-number">1.4.</span> <span class="toc-text">最近邻</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#决策树"><span class="toc-number">1.5.</span> <span class="toc-text">决策树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#支持向量机"><span class="toc-number">1.6.</span> <span class="toc-text">支持向量机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#朴素贝叶斯"><span class="toc-number">1.7.</span> <span class="toc-text">朴素贝叶斯</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#集成方法"><span class="toc-number">1.8.</span> <span class="toc-text">集成方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#—"><span class="toc-number">2.</span> <span class="toc-text">—-</span></a></li></ol></div></div><div class="post-content"><p><strong>—loading …—</strong><br>scikit-learn 基于 Python 语言,建立在 NumPy ，SciPy 和 matplotlib 上,是简单高效的数据挖掘和数据分析工具<br>文章记录 scikit-learn API的使用方法，包括监督学习、无监督学习，模型的选择和评估，数据集的加载和转换<br>学习笔记(1) 为监督学习的内容，解决回归和分类问题。包括线性回归，逻辑回归，梯度下降，最近邻，贝叶斯，决策树，支持向量机，集成方法，半监督学习等<br>API只给出函数结构，参数具体使用参考<a href="https://scikit-learn.org/stable/user_guide.html" target="_blank" rel="noopener">scikit-learn官方文档</a><br><a id="more"></a></p>
<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><h3 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h3><p>目标值 y 是输入变量 x 的线性组合 $y(w,x) = w_0 +w_1x_1 + … + w_px_p$ 其中w为系数(coef) $w_0$ 为截距(intercept)。</p>
<p><strong>1. 普通最小二乘法 LinearRegression</strong><br>拟合一个带有系数 $w = (w_1, …, w_p)$ 的线性模型，使得数据集实际观测数据和预测数据（估计值）之间的残差平方和最小。<br>$\underset{w}{min\,} {|| X w - y||_2}^2$</p>
<p><code>sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">reg = LinearRegression().fit(X,y) <span class="comment"># 模型拟合</span></span><br><span class="line">reg.cof_  <span class="comment"># 得到系数矩阵</span></span><br><span class="line">reg.intercept_ <span class="comment"># 得到截距矩阵</span></span><br><span class="line">reg.score(X,y) <span class="comment"># 误差</span></span><br><span class="line">reg.predict(X) <span class="comment"># 预测</span></span><br></pre></td></tr></table></figure>
<p><strong>2. 岭回归 Ridge</strong><br>回归通过对系数的大小<strong>施加惩罚</strong>来解决普通最小二乘法的一些问题。 岭系数最小化的是带罚项的残差平方和。<br>$\underset{w}{min\,} {||X w - y||_2^2 + \alpha ||w||_2^2}$，其中$\alpha \geq 0$ 是控制系数收缩量的复杂性参数(超参数)</p>
<p><code>sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=’auto’, random_state=None)</code></p>
<p>// alpha 可以设超参数大小<br>// tol 设置迭代最小边界<br>// solver : {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’} 设置求解器，拟合数据时使用的算法</p>
<ul>
<li><code>sklearn.linear_model.RidgeCV( )</code> 内置对$\alpha$的交叉验证实现岭回归</li>
</ul>
<p><strong>3. 套索回归 Lasso</strong><br>用于拟合<strong>稀疏系数</strong>的线性模型，使用了 coordinate descent （坐标下降算法）来拟合系数，罚项 为L1 范数。<br>$\underset{w}{min\,} { \frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \alpha ||w||_1}$</p>
<p><code>sklearn.linear_model.Lasso(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection=’cyclic’)</code></p>
<ul>
<li><code>sklearn.linear_model.LassoCV()</code> // 对超参数$\alpha$采用交叉验证</li>
<li><code>sklearn.linear_model.LassoLars()</code> // 采用的是最小角回归法，而不是坐标下降法进行优化</li>
<li><code>sklearn.linear_model.LassoLarsCV()</code> //同时采用不同的验证和优化方法</li>
<li><p><code>sklearn.linear_model.LassoLarsIC()</code> //对超参数$\alpha$采用 Akaike 信息准则(AIC)和贝叶斯信息准则(BIC),需要假设模型是正确的，对大样本（渐近结果）进行导出</p>
</li>
<li><p>LassoLarsCV 在寻找 $\alpha$ 参数值上更具有优势，而且如果样本数量比特征数量少得多时，通常 LassoLarsCV 比 LassoCV 要快</p>
</li>
</ul>
<p><strong>4. 弹性网络 ElasticNet</strong><br>Lasso和Ridge结合，对普通的线性回归做了正则化，但是它的损失函数既不全是L1的正则化，也不全是L2的正则化，而是用一个权重参数ρ来平衡L1和L2正则化的比重<br>适合只有少量参数是非零稀疏的模型<br>$\underset{w}{min\,} { \frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \alpha \rho ||w||_1 +\frac{\alpha(1-\rho)}{2} ||w||_2 ^ 2}$</p>
<p><code>sklearn.linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection=’cyclic’)</code></p>
<ul>
<li><code>ElasticNetCV()</code> 可以通过交叉验证来设置参数 $\alpha$ 和 $\rho$</li>
</ul>
<p><strong>5. Multitask 多任务</strong><br>可以实现多元回归稀疏模型的预测，即多个线性模型共同拟合<br>有<code>MultiTaskLasso()</code>,<code>MultiTaskLassoCV()</code>,<code>MultiTaskElasticNet()</code>,<code>MultiTaskElasticNetCV()</code></p>
<p><strong>6. 正交匹配追踪法（OMP）</strong><br><code>OrthogonalMatchingPursuit( )</code><br><code>OrthogonalMatchingPursuitCV( )</code></p>
<p><strong>7. 贝叶斯回归</strong></p>
<p><strong>8. 稳健回归</strong></p>
<p><strong>9. 多项式回归</strong></p>
<h3 id="logistic-线性分类模型"><a href="#logistic-线性分类模型" class="headerlink" title="logistic 线性分类模型"></a>logistic 线性分类模型</h3><p>解决分类问题的线性模型，将单次实验的结果输出为概率进行分类<br>$C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) $<br>可以选择L1，L2或Elastic-Net正则化进行约束</p>
<p><code>sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)</code><br>// penalty 参数可以设置正则化参数<br>// C 用$1/{\lambda}$表示的正则化强度参数<br>// solver : str, {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’} 选择优化算法，详细使用说明见官方文档<br>// class_weight 可以设置各类型的权重<br>// l1_ration 设置Elastic-Net中ρ控制正则化L1与正则化L2的强度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">reg = LogisticRegression().fit(X,y) <span class="comment"># 模型拟合</span></span><br><span class="line">reg.cof_  <span class="comment"># 得到系数矩阵</span></span><br><span class="line">reg.intercept_ <span class="comment"># 得到截距矩阵</span></span><br><span class="line">reg.score(X,y) <span class="comment"># 误差</span></span><br><span class="line">reg.predict(X) <span class="comment"># 得到预测的分类矩阵</span></span><br><span class="line">reg.predict_proba(X) <span class="comment"># 得到预测的概率</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>LogisticRegressionCV( )</code> 实现了内置交叉验证，可以找出最优的C和l1_ratio参数</li>
</ul>
<h3 id="SGD-随机梯度下降"><a href="#SGD-随机梯度下降" class="headerlink" title="SGD 随机梯度下降"></a>SGD 随机梯度下降</h3><p>可以拟合线性的回归和分类模型，在样本量很大时尤为有用，选择和函数时要避免过拟合。<br><code>SGDClassifier()</code> 和 <code>SGDRegressor()</code> 分别用于拟合分类问题和回归问题的线性模型，可使用不同的（凸）损失函数，支持不同的惩罚项。</p>
<h3 id="最近邻"><a href="#最近邻" class="headerlink" title="最近邻"></a>最近邻</h3><p>能够应用于决策边界非常不规则的分类情景</p>
<p>NearestNeighbors<br>BallTree<br>KDTree</p>
<p>最近邻分类属于<strong>基于实例的学习</strong>或<strong>非泛化学习</strong>：它不会去构造一个泛化的内部模型，而是简单地存储训练数据的实例。</p>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>决策树便于理解和解释，能够处理数值型数据和分类数据；但是容易产生一个过于复杂的模型，泛化性能会很差，结果不稳定，可以通过决策树的集成来得到缓解。如果某些类在问题中占主导地位会使得创建的决策树有偏差，建议在拟合前先对数据集进行平衡。</p>
<p>分类标准为最小化交叉熵（ID3），熵增益（C4.5）或基尼系数（Cart）</p>
<p><code>sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)</code><br>// criterion 选择分类标准，可以使用”gini”或者”entropy”<br>// splitter 设置特征划分标准<br>// max_feature 划分的最大特征数<br>// max_depth 选择树的最大深度<br>// min_sample_split 设置最小样本分类<br>// min_sample_leaf 设置最小样本叶子数<br>// max_leaf_nodes 最大叶子节点数<br>// class_weight 设置类别权重<br>// presort 设置数据预排序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">clf = DecisionTreeClassifier().fit(X,y)</span><br><span class="line">clf.n_class_ <span class="comment"># 分类数量</span></span><br><span class="line">clf.n_features_ <span class="comment"># 特征数量</span></span><br><span class="line">clf.n_outputs_ <span class="comment"># 输出数量</span></span><br><span class="line">clf.tree_ <span class="comment"># 得到分类树</span></span><br><span class="line">clf.get_depth() <span class="comment"># 得到分类深度</span></span><br><span class="line">clf.get_n_leaves() <span class="comment"># 得到叶子节点数</span></span><br><span class="line">clf.predict(X) <span class="comment"># 返回预测矩阵</span></span><br><span class="line">clf.predict_proba(X) <span class="comment"># 返回预测的概率</span></span><br><span class="line">clf.score(X,y) <span class="comment"># 返回训练分数</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>决策树回归模型 <code>sklearn.tree.DecisionTreeRegressor()</code> （不常用）</p>
</li>
<li><p>可以使用 export_graphviz 导出器以 Graphviz 格式导出决策树,结果保存为pdf；Jupyter notebook也可以自动内联式渲染这些绘制节点</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line">dot_data = tree.export_graphviz(clf, out_file=<span class="keyword">None</span>,\</span><br><span class="line">                    feature_names=iris.feature_names,\  </span><br><span class="line">                    class_names=iris.target_names,  \</span><br><span class="line">                    filled=<span class="keyword">True</span>, rounded=<span class="keyword">True</span>,  \</span><br><span class="line">                    special_characters=<span class="keyword">True</span>)  </span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line">graph.render(<span class="string">"classification_result"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># show in Jupyter</span></span><br><span class="line">graph</span><br></pre></td></tr></table></figure>
<h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><p>可用于分类，回归和异常检测，在高维空间中非常高效，<br>SVC, NuSVC 和 LinearSV</p>
<p>内核岭回归<br>核函数</p>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>GaussianNB<br>MultinomialNB<br>ComplementNB<br>BernoulliNB</p>
<h3 id="集成方法"><a href="#集成方法" class="headerlink" title="集成方法"></a>集成方法</h3><p><strong>1. Bagging</strong><br>在原始训练集的随机子集上构建一类黑盒估计器的多个实例，然后把这些估计器的预测结果结合起来形成最终的预测结果，在构建模型的过程中引入随机性，来减少基估计器的方差。</p>
<p><code>sklearn.ensemble.BaggingClassifier(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)</code><br>// base_estimator 设置分类估计器，默认为决策树，可设置其它分类模型<br>// n_estimators 设置估计器的数量<br>// bootstrap 设置样例抽取是否放回<br>// bootstrap_features 设置特征抽取是否有放回<br>// oob_score 设置是否用额外的样本来评估泛化精度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line">clf = BaggingClassifier().fit(X,y)</span><br><span class="line">clf.predict(X)</span><br><span class="line">predict_proba(X)</span><br><span class="line">score(X, y)</span><br></pre></td></tr></table></figure>
<p><strong>2. 随机森林</strong><br>随机森林中的每棵树构建时的样本都是由训练集经过有放回抽样得到。在构建树的过程中进行结点分割时，选择的分割点不再是所有特征中最佳分割点，而是特征的一个随机子集中的最佳分割点，偏差通常会有略微的增大；但由于取了平均，总体上模型的泛化能力会更好。</p>
<p><code>sklearn.ensemble.RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)</code><br>// n_estimators 调整森林里树的数量<br>// max_features 分割节点时考虑的特征的随机子集的大小。使用 max_features = sqrt(n_features)是比较好的默认值<br>// max_depth = None 和 min_samples_split = 2 结合可以生成完全树</p>
<ul>
<li>极限随机树 <code>ExtraTreesClassifier()</code></li>
<li>完全随机树构成森林 <code>RandomTreesEmbedding()</code></li>
</ul>
<p><strong>2. 梯度提升回归树(GBRT)</strong><br>GBRT保证迭代的每一个阶段中选择损失最小的决策树，达到全局的最小损失<br>$F<em>m(x) = F</em>{m-1}(x) + \arg\min<em>{h} \sum</em>{i=1}^{n} L(y<em>i,F</em>{m-1}(x_i) - h(x))$<br>GBRT具有强大的预测能力和鲁棒性，但是扩展性不够好</p>
<p><code>GradientBoostingClassifier(loss=’deviance’, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=’friedman_mse’, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort=’auto’, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)</code><br>// loss 设置损失函数，默认为deviance（$-log_2$ 似然损失函数）<br>// learning_rate 设置学习步长<br>// subsample 设置每次子训练集的采样，1为全采样，一般可设置为0.5</p>
<p><strong>3. AdaBoost</strong><br>通过反复修改数据权重来训练一些弱学习器，由这些弱学习器的预测结果通过加权投票(或加权求和)的方式组合, 得到最终的预测结果。<br>$F<em>m(x) = F</em>{m-1}(x) + \gamma<em>m \sum</em>{i=1}^{n} \nabla<em>F L(y_i, F</em>{m-1}(x<em>i))$<br>$\gamma_m$代表学习步长，通过计算损失最小的梯度下降$\gamma_m = \arg\min</em>{\gamma} \sum<em>{i=1}^{n} L(y_i, F</em>{m-1}(x<em>i) - \gamma \frac{\partial L(y_i, F</em>{m-1}(x<em>i))}{\partial F</em>{m-1}(x_i)})$得出</p>
<p>初始化时，将所有弱学习器的权重都设置为 $w_i = 1/N$ ,接下来的连续迭代中，样本的权重逐个地被修改。上一轮迭代中被预测为错误结果的样本的权重将会被增加，而那些被预测为正确结果的样本的权重将会被降低。通过不断得到最快梯度下降的权重来减小损失函数。</p>
<p><code>sklearn.ensemble.AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=’SAMME.R’, random_state=None)</code><br>// base_estimator 设置弱学习器，默认DecisionTreeClassifier(max_depth=1)<br>// n_estimator 设置最大集成数量<br>// learning_rate 设置学习速率</p>
<p><strong>4. 投票分类器</strong></p>
<h2 id="—"><a href="#—" class="headerlink" title="—-"></a>—-</h2><ul>
<li>最小角回归（LARS）：逐步寻找与响应最有关联的预测。当有很多预测有相同的关联时，它并不会继续利用相同的预测，而是在这些预测中找出应该等角的方向。高效但对噪声敏感。</li>
</ul>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>KevinW</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2019/07/30/scikit-learn-note/">https://hyqskevin.github.io/2019/07/30/scikit-learn-note/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</li></ul></div><br><div class="tags"><a href="../../../../tags/python/">python</a></div><div class="post-nav"><a class="pre" href="../../../08/01/boosting/">boosting</a><a class="next" href="../../../04/11/flask-learning2/">Flask Web 学习笔记2 -- URL与视图函数映射</a></div></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 学习分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/code/">code</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/language-learning/">language-learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/notes/">notes</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/paper/">paper</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/repo/">repo</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/study/">study</a><span class="category-list-count">21</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="../../../../tags/python/" style="font-size: 15px;">python</a> <a href="../../../../tags/algorithm/" style="font-size: 15px;">algorithm</a> <a href="../../../../tags/book/" style="font-size: 15px;">book</a> <a href="../../../../tags/others/" style="font-size: 15px;">others</a> <a href="../../../../tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="../../../../tags/time-series/" style="font-size: 15px;">time_series</a> <a href="../../../../tags/ensemble-learning/" style="font-size: 15px;">ensemble_learning</a> <a href="../../../../tags/bp-nn/" style="font-size: 15px;">bp_nn</a> <a href="../../../../tags/flask/" style="font-size: 15px;">flask</a> <a href="../../../../tags/hexo/" style="font-size: 15px;">hexo</a> <a href="../../../../tags/bootstrap2-0/" style="font-size: 15px;">bootstrap2.0</a> <a href="../../../../tags/php/" style="font-size: 15px;">php</a> <a href="../../../../tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="../../../../tags/mysql/" style="font-size: 15px;">mysql</a> <a href="../../../../tags/session/" style="font-size: 15px;">session</a> <a href="../../../../tags/cookie/" style="font-size: 15px;">cookie</a> <a href="../../../../tags/slam/" style="font-size: 15px;">slam</a> <a href="../../../../tags/Linux-device/" style="font-size: 15px;">Linux_device</a> <a href="../../../../tags/log/" style="font-size: 15px;">log</a> <a href="../../../../tags/c/" style="font-size: 15px;">c++</a> <a href="../../../../tags/laravel/" style="font-size: 15px;">laravel</a> <a href="../../../../tags/machine-learning/" style="font-size: 15px;">machine_learning</a> <a href="../../../../tags/pandas/" style="font-size: 15px;">pandas</a> <a href="../../../../tags/boosting/" style="font-size: 15px;">boosting</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="../../../08/20/imbalanced_data_analysis/">imblearn API</a></li><li class="post-list-item"><a class="post-list-link" href="../../../08/10/decision-tree-visualization/">tip:决策树、随机森林结果可视化</a></li><li class="post-list-item"><a class="post-list-link" href="../../../08/04/write_log/">如何写日志</a></li><li class="post-list-item"><a class="post-list-link" href="../../../08/01/boosting/">boosting</a></li><li class="post-list-item"><a class="post-list-link" href>scikit-learn 文档学习笔记(1)</a></li><li class="post-list-item"><a class="post-list-link" href="../../../04/11/flask-learning2/">Flask Web 学习笔记2 -- URL与视图函数映射</a></li><li class="post-list-item"><a class="post-list-link" href="../../../04/05/ptr/">c++中的智能指针</a></li><li class="post-list-item"><a class="post-list-link" href="../../../03/31/flask-learning1/">Flask Web 学习笔记1 -- 环境</a></li><li class="post-list-item"><a class="post-list-link" href="../../../03/25/cpp-oop/">c++ 面向对象的一些特性</a></li><li class="post-list-item"><a class="post-list-link" href="../../../03/24/bintree/">二叉树的相关操作</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="../../../../." rel="nofollow">MonoShow.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../../../../js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../../../../js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/javascript" src="../../../../js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="0,0,0" opacity="0.5" zindex="-2" count="50" src="//lib.baomitu.com/canvas-nest.js/2.0.3/canvas-nest.umd.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="../../../../js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="../../../../js/smartresize.js?v=0.0.0"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"left","width":100,"height":200},"mobile":{"show":true},"log":false});</script></body></html>