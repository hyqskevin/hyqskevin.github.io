<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Kevin W build this air-castle to collect infos &amp; photos"><title>boosting | MonoShow</title><link rel="stylesheet" type="text/css" href="../../../../css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask sizes="any" href="../../../../favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="../../../../favicon.ico"><link rel="apple-touch-icon" href="../../../../apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="../../../../apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="../../../../atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">boosting</h1><a id="logo" href="../../../../.">MonoShow</a><p class="description">Monologue from Kevin_W</p></div><div id="nav-menu"><a class="current" href="../../../../."><i class="fa fa-home"> 时间线</i></a><a href="../../../../archives/"><i class="fa fa-archive"> 档案袋</i></a><a href="../../../../categories/language-learning/"><i class="fa fa-book"> 看原著了嘛</i></a><a href="../../../../categories/code/"><i class="fa fa-github"> 码代码了嘛</i></a><a href="../../../../categories/paper/"><i class="fa fa-file-text"> 翻译论文了嘛</i></a><a href="../../../../photo/"><i class="fa fa-camera"> Ins摄影集</i></a><a href="../../../../about/"><i class="fa fa-user"> 我是谁我在哪</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">boosting</h1><div class="post-meta">Aug 1, 2019<span> | </span><span class="category"><a href="../../../../categories/study/">study</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 4.1k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 17</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p><a href="https://xgboost.ai/" target="_blank" rel="noopener">XgBoost</a> 和 <a href="https://github.com/Microsoft/LightGBM" target="_blank" rel="noopener">LightGBM</a> 官方文档阅读和算法学习<br><code>*</code>号为未解释完全，具体使用方法请参考官方文档</p>
<a id="more"></a>
<h2 id="XgBoost"><a href="#XgBoost" class="headerlink" title="XgBoost"></a>XgBoost</h2><h3 id="常用参数"><a href="#常用参数" class="headerlink" title="常用参数"></a>常用参数</h3><p><code>booster</code> [default= gbtree ]  基础模型类型，可选参数包括: gbtree、gblinear、dart，其中 gbtree、dart 为树模型、gblinear 为线性函数模型</p>
<p><code>eta</code> 学习率</p>
<p><code>tree_method</code> XGBoost 中树的构造算法，可选项包括: auto, exact, approx, hist, gpu_exact, gpu_hist</p>
<p><code>eval_metric</code> 依据目标函数选择评估指标<br>rmse: 根均方误差<br>mae: 平均绝对值误差<br>logloss: 负的似然函数<br>error: 二分类问题的分类错误率<br>merror: 多分类问题的分类错误率<br>mlogloss: 多分类问题的负似然函数<br>auc: IOC 曲线下面积<br>aucpr: PR 曲线下面积</p>
<p><code>updater</code> 线性模型的拟合算法<br>shotgun: 基于 shotgun 算法的坐标下降法<br>coord_descent: 普通的坐标下降法<br>feature_selector: 特征选择和排序算法</p>
<p><code>objective</code> 训练的模型类型和目标函数<br>reg:linear: 线性回归<br>reg:logistic: 逻辑回归<br>binary:logistic: 二分类问题, 输出概率值<br>binary:logitraw: 二分类问题, 输出得分值，需要通过 sigmoid 函数转化成概率值<br>binary:hinge: 二分类问题，使用铰链损失函数,输出 0 或 1，而不是概率值<br>count:poisson: 用于计数问题的泊松分布，输出泊松分布的均值。<br>survival:cox: Cox regression for right censored survival time data<br>multi:softmax: 多分类目标函数, 使用此目标函数，需要设置样本类别数据： num_class<br>multi:softprob: 同 softmax, 但是输出的结果为 ndata * nclass 维的向量，表示样本属于每个类别的概率</p>
<p><code>cyclic</code>: 循环变量特征<br><code>shuffle</code>: 类型与循环变量特征，但是在每次更新时都会随机打乱特征的顺序<br><code>random</code>: 随机(带替换)的坐标选择器<br><code>greedy</code>: 选择最大梯度的坐标<br><code>thrifty</code>: 近似 greedy 的坐标选择器<br><code>top_k</code>: greedy 算法和 thrifty 算法选择的最优特征数量，0 表示不限制。</p>
<h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><p><strong>1. 数据结构类，提供数据的构建和处理</strong><br><code>xgboost.DMatrix(data, label=None, missing=None, weight=None, silent=False, feature_names=None, feature_types=None)</code></p>
<p><code>data</code>数据源或文件路径<br><code>label</code>训练数据的标签<br><code>missing</code>缺省值表示字符，如果没填, 默认值为：np.nan<br><code>weight</code>每个样本的权重<br><code>silent</code>构造数据结构时是否显示日志<br><code>feature_names</code>各个特征的名称<br><code>feature_types</code>各个特征的数据类型<br><code>nthread</code> 加载数据开启的线程数</p>
<p><strong>2. 模型类，提供了一些基础的函数，如模型加载、保存、评估和预测等方法</strong><br><code>xgboost.Booster(params=None, cache=(), model_file=None)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">load_model(fname)</span><br><span class="line">从文件或内存中加载模型，参数含义如下：</span><br><span class="line"></span><br><span class="line">fname (string <span class="keyword">or</span> a memory buffer) – 模型文件名称或内存缓存对象</span><br><span class="line"><span class="number">2.</span> save_model(fname)</span><br><span class="line">将模型保存到文件中，参数的含义如下：</span><br><span class="line"></span><br><span class="line">fname (string) – 输出文件的名称</span><br><span class="line"><span class="number">3.</span> eval(data, name, iteration)</span><br><span class="line">用给定的数据评估模型好坏，参数的含义如下:</span><br><span class="line"></span><br><span class="line">data (DMatrix) – 用于评估模型的数据</span><br><span class="line"></span><br><span class="line">name (str, 可选) – 用于评估模型的数据集名称</span><br><span class="line"></span><br><span class="line">iteration (int, 可选) –迭代次数</span><br></pre></td></tr></table></figure>
<p><strong>3. 对模型进行训练</strong><br><code>xgboost.train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True, learning_rates=None, xgb_model=None, callbacks=None)</code></p>
<p><code>params</code>配置参数<br><code>dtrain</code>训练数据<br><code>num_boost_round</code>生成树的数量<br><code>evals</code>评估数据<br><code>obj</code>自定义的目标函数<br><code>feval</code>自定义的评价函数<br><code>maximize</code>是否最大化评价指标<br><code>early_stopping_rounds</code>错误率 early_stopping_rounds 轮未下降，则停止训练<br><code>evals_result</code>模型评估结果<br><code>learning_rates</code>学习率<br><code>xgb_model</code>在训练前加载之前训练的模型<br><code>callback</code>设置回调函数，比如重新设置学习率</p>
<p><strong>4. 对数据分类</strong><br><code>xgboost.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective=&#39;binary:logistic&#39;, nthread=-1, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, seed=0, missing=None)</code></p>
<p><code>max_depth</code> 最大树深度<br><code>learning_rate</code> 学习率<br><code>n_estimators</code> 树的迭代次数<br><code>gamma</code> 节点分裂需要下降的最小损失<br><code>min_child_weight</code> 节点中样本的最小权重和<br><code>max_delta_step</code> 每轮允许叶子输出值的最大增量<br><code>subsample</code> 每轮训练使用的样本数量等于样本总数乘以采样率<br><code>colsample_bytree</code> 每轮训练使用的特征占比<br><code>colsample_bylevel</code> 每层训练使用的特征占比<br><code>reg_alpha</code> L1 正则<br><code>reg_lambda</code> L2 正则<br><code>scale_pos_weight</code> 用于控制正例和负例均衡的权重<br><code>base_score</code> 初始时各个样本的得分</p>
<p><strong>5. 数据拟合和预测</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xgboost.fit(X, y, sample_weight=<span class="keyword">None</span>, eval_set=<span class="keyword">None</span>, eval_metric=<span class="keyword">None</span>, early_stopping_rounds=<span class="keyword">None</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># sample_weight：每个训练样本的权重</span></span><br><span class="line"><span class="comment"># eval_set：设置验证集</span></span><br><span class="line"><span class="comment"># eval_metric：验证的度量指标</span></span><br><span class="line"><span class="comment"># early_stopping_rounds` 指定连续多少轮未改善后停止</span></span><br><span class="line"></span><br><span class="line">xgboost.predict(data, output_margin=<span class="keyword">False</span>, ntree_limit=<span class="number">0</span>, pred_leaf=<span class="keyword">False</span>, pred_contribs=<span class="keyword">False</span>, approx_contribs=Flase, pred_interactions=<span class="keyword">False</span>, validate_features=Flase)`</span><br><span class="line"></span><br><span class="line"><span class="comment"># output_margin 是否输出原始未经转化的值</span></span><br><span class="line"><span class="comment"># ntree_limit 用于预测的树的数量，默认为0，代表使用所有树进行预测</span></span><br><span class="line"><span class="comment"># pred_leaf 指明每条数据分别落到每棵树的哪个叶子节点上</span></span><br><span class="line"><span class="comment"># pred_contribs 指明每个样本的每个特征对预测结果的贡献值</span></span><br><span class="line"><span class="comment"># approx_contribs 是否启用特征贡献大小的预估功能</span></span><br><span class="line"><span class="comment"># pred_interactions 指明两两特征间相互影响的SHAP值</span></span><br><span class="line"><span class="comment"># validate_features首先验证待预测的数据特征名称是否与模型中的特征名称相同，默认情况下，系统认为他们是相同的，不进行验证</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>class xgboost.XGBRegressor()</code>用于回归任务</li>
</ul>
<h3 id="XgBoost-示例"><a href="#XgBoost-示例" class="headerlink" title="XgBoost 示例"></a>XgBoost 示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train.csv'</span>)   <span class="comment"># 读取数据</span></span><br><span class="line">y = train_data.pop(<span class="string">'30'</span>).values   <span class="comment"># 用pop方式将训练数据中的标签值y取出来，作为训练目标，这里的‘30’是标签的列名</span></span><br><span class="line">col = train_data.columns</span><br><span class="line">x = train_data[col].values  <span class="comment"># 剩下的列作为训练数据</span></span><br><span class="line">train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=<span class="number">0.333</span>, random_state=<span class="number">0</span>)   <span class="comment"># 分训练集和验证集</span></span><br><span class="line"><span class="comment"># 这里不需要Dmatrix</span></span><br><span class="line"></span><br><span class="line">parameters = &#123;</span><br><span class="line">              <span class="string">'max_depth'</span>: [<span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">25</span>],</span><br><span class="line">              <span class="string">'learning_rate'</span>: [<span class="number">0.01</span>, <span class="number">0.02</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.15</span>],</span><br><span class="line">              <span class="string">'n_estimators'</span>: [<span class="number">500</span>, <span class="number">1000</span>, <span class="number">2000</span>, <span class="number">3000</span>, <span class="number">5000</span>],</span><br><span class="line">              <span class="string">'min_child_weight'</span>: [<span class="number">0</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">              <span class="string">'max_delta_step'</span>: [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">              <span class="string">'subsample'</span>: [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.85</span>, <span class="number">0.95</span>],</span><br><span class="line">              <span class="string">'colsample_bytree'</span>: [<span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>],</span><br><span class="line">              <span class="string">'reg_alpha'</span>: [<span class="number">0</span>, <span class="number">0.25</span>, <span class="number">0.5</span>, <span class="number">0.75</span>, <span class="number">1</span>],</span><br><span class="line">              <span class="string">'reg_lambda'</span>: [<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>],</span><br><span class="line">              <span class="string">'scale_pos_weight'</span>: [<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>]</span><br><span class="line">&#125;</span><br><span class="line">xlf = xgb.XGBClassifier(max_depth=<span class="number">10</span>,</span><br><span class="line">            learning_rate=<span class="number">0.01</span>,</span><br><span class="line">            n_estimators=<span class="number">2000</span>,</span><br><span class="line">            silent=<span class="keyword">True</span>,</span><br><span class="line">            objective=<span class="string">'binary:logistic'</span>,</span><br><span class="line">            nthread=<span class="number">-1</span>,</span><br><span class="line">            gamma=<span class="number">0</span>,</span><br><span class="line">            min_child_weight=<span class="number">1</span>,</span><br><span class="line">            max_delta_step=<span class="number">0</span>,</span><br><span class="line">            subsample=<span class="number">0.85</span>,</span><br><span class="line">            colsample_bytree=<span class="number">0.7</span>,</span><br><span class="line">            colsample_bylevel=<span class="number">1</span>,</span><br><span class="line">            reg_alpha=<span class="number">0</span>,</span><br><span class="line">            reg_lambda=<span class="number">1</span>,</span><br><span class="line">            scale_pos_weight=<span class="number">1</span>,</span><br><span class="line">            seed=<span class="number">1440</span>,</span><br><span class="line">            missing=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 有了gridsearch我们便不需要fit函数</span></span><br><span class="line">gsearch = GridSearchCV(xlf, param_grid=parameters, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">3</span>)</span><br><span class="line">gsearch.fit(train_x, train_y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Best score: %0.3f"</span> % gsearch.best_score_)</span><br><span class="line">print(<span class="string">"Best parameters set:"</span>)</span><br><span class="line">best_parameters = gsearch.best_estimator_.get_params()</span><br><span class="line"><span class="keyword">for</span> param_name <span class="keyword">in</span> sorted(parameters.keys()):</span><br><span class="line">    print(<span class="string">"\t%s: %r"</span> % (param_name, best_parameters[param_name]))</span><br></pre></td></tr></table></figure>
<h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h2><h3 id="paramenter-常用参数"><a href="#paramenter-常用参数" class="headerlink" title="paramenter 常用参数"></a>paramenter 常用参数</h3><p><strong>objective</strong>:<br>‘regression’,’regression_l2’,’mean_squared_error’,’mse’,’l2_root’,’root_mean_squred_error’,’rmse’： 表示回归任务，但是使用 L2 损失函数。默认为’regression’<br>‘binary’： 表示二分类任务，使用对数损失函数作为目标函数。<br>‘multiclass’： 表示多分类任务，使用 softmax 函数作为目标函数。必须设置 num_class 参数<br>‘multiclassova’,’multiclass_ova’,’ova’,’ovr’： 表示多分类任务，使用 one-vs-all 的二分类目标函数。必须设置 num_class 参数<br>‘regression_l1’,’mae’,’mean_absolute_error’： 表示回归任务，但是使用 L1 损失函数。<br>‘huber’： 表示回归任务，但是使用 huber 损失函数。<br>‘fair’： 表示回归任务，但是使用 fair 损失函数。<br>‘poisson’： 表示 Poisson 回归任务。<br>‘quantile’： 表示 quantile 回归任务。<br>‘quantile_l2’：表示 quantile 回归任务，但是使用了 L2 损失函数。<br>‘mape’,’mean_absolute_precentage_error’： 表示回归任务，但是使用 MAPE 损失函数<br>‘gamma’： 表示 gamma 回归任务。<br>‘tweedie’： 表示 tweedie 回归任务。<br>‘xentropy’,’cross_entropy’： 目标函数为交叉熵（同时具有可选择的线性权重）。要求标签是[0,1] 之间的数值。<br>‘xentlambda’,’cross_entropy_lambda’： 替代了参数化的 cross_entropy 。要求标签是[0,1] 之间的数值。<br>‘lambdarank’：表示排序任务。</p>
<p><strong>boosting_type</strong>: // 基学习器模型算法<br>‘gbdt’： 表示传统的梯度提升决策树。默认值为’gbdt’<br>‘rf’： 表示随机森林。<br>‘dart’： 表示带 dropout 的 gbdt<br>‘goss’：表示 Gradient-based One-Side Sampling 的 gbdt</p>
<p><strong>metric</strong>：//指定度量的指标<br>‘l1’ 或者 mean_absolute_error 或者 mae 或者 regression_l1： 表示绝对值损失<br>‘l2’ 或者 mean_squared_error 或者 mse 或者 regression_l2 或者 regression：表示平方损失<br>‘l2_root’ 或者 root_mean_squared_error 或者 rmse：表示开方损失<br>‘quantile’ 表示 Quantile 回归中的损失<br>‘mape’ 或者 ‘mean_absolute_percentage_error’ 表示 MAPE 损失<br>‘huber’ 表示 huber 损失<br>‘fair’ 表示 fair 损失<br>‘poisson’ 表示 poisson 回归的负对数似然<br>‘gamma’ 表示 gamma 回归的负对数似然<br>‘gamma_deviance’ 表示 gamma 回归的残差的方差<br>‘tweedie’ 表示 Tweedie 回归的负对数似然<br>‘ndcg’ 表示 NDCG<br>‘map’ 或者’mean_average_precision’ 表示平均的精度<br>‘auc’ 表示 AUC<br>‘binary_logloss’或者’binary’ 表示二类分类中的对数损失函数<br>‘binary_error’ 表示二类分类中的分类错误率<br>‘multi_logloss’或者 ‘multiclass’或者 ‘softmax’或者 ‘multiclassova’或者 ‘multiclass_ova’,或者’ova’或者 ‘ovr’ 表示多类分类中的对数损失函数<br>‘multi_error’ 表示多分类中的分类错误率<br>‘xentropy’或者’cross_entropy’ 表示交叉熵<br>‘xentlambda’ 或者’cross_entropy_lambda’ 表示 intensity 加权的交叉熵<br>‘kldiv’或者’kullback_leibler’ 表示 KL 散度</p>
<h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><p><strong>1. 数据结构类，提供数据的构建和处理</strong><br><code>lightgbm.Dataset(data, label=None, max_bin=None, reference=None, weight=None, group=None, init_score=None, silent=False, feature_name=&#39;auto&#39;, categorical_feature=&#39;auto&#39;, params=None, free_raw_data=True)</code></p>
<p><code>label</code> 指定数据的标签列<br><code>max_bin</code> 特征值最大分类数量<br><code>reference</code> 增加评估参照，评估模型时使用，reference=train<br><code>weight</code> 设置权重<br><code>group</code> 设置数据组的大小<br><code>init_score</code> 加入之前的分数<br><code>silent</code> 是否在训练过程中屏蔽输出<br><code>feature_name</code> 特征名字<br><code>categorical_feature</code>设置分类特征<br><code>free_raw_data</code> 创建完后释放数据</p>
<p><strong>2. 模型类，提供了一些基础的函数，如模型加载、保存、评估和预测等方法</strong><br><code>lightgbm.Booster(params=None, train_set=None, model_file=None, model_str=None, silent=False)</code><br><code>params</code> 一个字典或者 None，给出了 Booster 的参数。默认为 None<br><code>train_set</code> 一个 Dataset 对象或者 None，给出了训练集。 默认为 None<br><code>model_file</code> 一个字符串或者 None，给出了 model file 的路径。 默认为 None<br><code>silent</code> 一个布尔值，指示是否在构建过程中打印消息。默认为 False</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lightgbm.add_valid(data,name) <span class="comment"># 添加一个验证集。</span></span><br><span class="line">lightgbm.current_iteration() <span class="comment"># 返回当前的迭代的index（即迭代的编号）</span></span><br><span class="line">lightgbm.dump_model(num_iteration=<span class="number">-1</span>) <span class="comment"># dump 当前的Booster 对象为json 格式。</span></span><br><span class="line">lightgbm.eval(data,name,feval=<span class="keyword">None</span>) <span class="comment"># 对指定的数据集evaluate</span></span><br><span class="line">lightgbm.eval_train(feval=<span class="keyword">None</span>) <span class="comment"># 对训练集进行evaluate</span></span><br><span class="line">lightgbm.eval_valid(feval=<span class="keyword">None</span>) <span class="comment"># 对验证集进行evaluate</span></span><br><span class="line">lightgbm.feature_importance(importance_type=<span class="string">'split'</span>, iteration=<span class="number">-1</span>) <span class="comment"># 获取特征的importance</span></span><br><span class="line">lightgbm.feature_name() <span class="comment"># 获取每个特征的名字。</span></span><br><span class="line">lightgbm.free_dataset() <span class="comment"># 释放Booster 对象的数据集</span></span><br><span class="line">lightgbm.free_network() <span class="comment"># 释放Booster 对象的Network</span></span><br><span class="line">lightgbm.get_leaf_output(tree_id, leaf_id) <span class="comment"># 获取指定叶子的输出</span></span><br><span class="line">lightgbm.num_feature() <span class="comment"># 获取特征的数量（即由多少列特征）</span></span><br><span class="line">lightgbm.reset_parameter(params) <span class="comment">#重设Booster 的参数。</span></span><br><span class="line">lightgbm.rollback_one_iter() <span class="comment"># 将Booster 回滚一个迭代步</span></span><br><span class="line">lightgbm.save_model(filename,num_iteration=<span class="number">-1</span>) <span class="comment"># 保存Booster 对象到文件中。</span></span><br><span class="line">lightgbm.set_attr(**kwargs) <span class="comment"># 设置Booster 的属性。</span></span><br><span class="line">lightgbm.set_network(machines,local_listen_port=<span class="number">12400</span>,listen_time_out=<span class="number">120</span>,num_machines=<span class="number">1</span>) <span class="comment"># 配置网络</span></span><br><span class="line">lightgbm.set_train_data_name(name) <span class="comment"># 设置训练集的名字</span></span><br><span class="line">lightgbm.update(train_set=<span class="keyword">None</span>, fobj=<span class="keyword">None</span>) <span class="comment"># 更新一个迭代步</span></span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p><strong>1. <code>lightgbm.train()</code>执行训练</strong></p>
<p><code>lightgbm.train(params, train_set, num_boost_round=100, valid_sets=None, valid_names=None, fobj=None, feval=None, init_model=None, feature_name=&#39;auto&#39;, categorical_feature=&#39;auto&#39;, early_stopping_rounds=None, evals_result=None, verbose_eval=True, learning_rates=None, keep_training_booster=False, callbacks=None)</code></p>
<p><code>params</code> 相关参数配置，另外导入字典<br><code>train_set</code> 训练数据<br><code>num_boost_round</code> boost 迭代次数<br><code>valid_sets</code>，<code>valid_names</code> 设置训练时用于评估的数据集 <em><br><code>fobj</code> 自定义目标函数 preds, train_data -&gt; grad,hess </em><br><code>feval</code> 自定义评估函数 preds, train<em>data -&gt; eval_name, eval_result, is_higher_better </em><br><code>init_model</code> 导入之前训练数据<br><code>feature_name</code> 指定特征名字，数据源为 pandas DataFrame 会使用里面的 column<em>names<br><code>categorical_feature</code> 指定分类特征<br><code>early_stopping_rounds</code> 指定连续多少轮未改善后停止<br><code>evals_result</code> 指定字典存储 valid_sets 中验证的结果 </em><br><code>verbose_eval</code> 设置打印评估的间隔，可设置每个提升阶段打印或间隔<code>verbose_eval</code>个阶段打印<br><code>learning_rates</code> 设置学习率<br><code>keep_training_booster</code> 设置训练得到的 booster 是否继续训练<br><code>callbacks</code> 设置每次迭代后需要执行的函数</p>
<p>最后返回 booster 实例</p>
<p><strong>2. <code>lightgbm.cv()</code> 执行交叉检验</strong></p>
<p><code>lightgbm.cv(params, train_set, num_boost_round=10, folds=None, nfold=5, stratified=True, shuffle=True, metrics=None, fobj=None, feval=None, init_model=None, feature_name=&#39;auto&#39;,categorical_feature=&#39;auto&#39;, early_stopping_rounds=None, fpreproc=None, verbose_eval=None, show_stdv=True, seed=0, callbacks=None)</code></p>
<p><code>folds</code>：一个生成器、一个迭代器、或者 None <em><br><code>nfold</code>：指定交叉检验的数量。默认为 5<br><code>stratified</code>：指示是否进行分层拆分<br><code>shuffle</code>：指示是否在拆分之前先混洗数据<br><code>metrics</code>：指定评估度量标准，在 params 中设置<br><code>fpreproc</code>：设置预处理函数，在训练开始之前进行 </em><br><code>show_stdv</code>：在训练过程中展示标准差信息<br><code>seed</code>：一个整数，给出了生成 fold 的随机数种子 *</p>
<p>最后以字典的形式返回检验结果的均值和标准差</p>
<h3 id="scikit-learn-API"><a href="#scikit-learn-API" class="headerlink" title="scikit-learn API"></a>scikit-learn API</h3><p><strong>1. LGBMMOdel</strong><br>实现 lightgbm 在 scikir-learn 中的接口，详见分类和回归任务</p>
<p><strong>2. LGBMClassifier: LGBMModel 的子类，用于分类任务</strong></p>
<p><code>lightgbm.LGBMClassifier(boosting_type=&#39;gbdt&#39;, num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=10, max_bin=255, ubsample_for_bin=200000, objective=None, min_split_gain=0.0, in_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=1, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=True, **kwargs)</code></p>
<p><code>boosting_type</code>： 指定学习器的算法’gbdt, rf, dart, goss’<br><code>num_leaves</code>：一棵树上的叶子数<br><code>max_depth</code>：树模型的最大深度<br><code>learning_rate</code>：学习率<br><code>n_estimators</code>：提升树的数量<br><code>max_bin</code>： 每个特征的最大分支数量<br><code>class_weight</code>：给出了每个类别的权重占比<br><code>subsample_for_bin</code>：构建直方图的样本的数量 <em><br><code>objective</code>：问题类型以及对应的目标函数，对于 LGBMRegressor 为’regression’；对于 LGBMClassifier 为’binary’ 或者’multiclass’；对于 LGBMRanker 为’lambdarank’<br><code>min_split_gain</code>：执行切分的最小增益<br><code>min_child_weight</code>：一个叶子节点上的最小权重之和，默认为 1e-3<br><code>min_child_samples</code>： 一个叶子节点上包含的最少样本数量<br><code>subsample</code>： 表示训练样本的采样比例，取值范围为[0.0,1.0]。如果小于 1.0，则 lightgbm 会在每次迭代中随机选择部分样本来训练（非重复采样）<br><code>subsample_freq</code>：表示训练样本的采样频率<br><code>colsample_bytree</code>：表示特征的采样比例，取值范围为[0.0,1.0]。如果小于 1.0，则 lightgbm 会在每次迭代中随机选择部分特征<br><code>reg_alpha</code>：L1 正则化系数<br><code>reg_lambda</code>：L2 正则化系数<br><code>random_state</code>：随机数种子 </em><br><code>n_jobs</code>：并行线程数量<br><code>silent</code>：是否在训练过程中屏蔽输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line">gbm = lgb.LGBMClassifier()</span><br><span class="line">gbm.n_features_：<span class="comment"># 给出了特征的数量</span></span><br><span class="line">gbm.classes_：<span class="comment"># 样本的标签</span></span><br><span class="line">gbm.n_classes_：<span class="comment"># 类别的数量</span></span><br><span class="line">gbm.best_score_：<span class="comment"># 训练完毕模型的最好的score</span></span><br><span class="line">gbm.best_iteration_：<span class="comment"># 训练完毕模型的最好的迭代数</span></span><br><span class="line">gbm.objective_：<span class="comment"># 训练模型的目标函数</span></span><br><span class="line">gbm.booster_：<span class="comment"># 底层的Booster 对象</span></span><br><span class="line">gbm.evals_result_：<span class="comment"># 模型评估结果</span></span><br><span class="line">gbm.feature_importances_： <span class="comment"># 特征的重要性</span></span><br></pre></td></tr></table></figure>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gbm.fit(X, y, sample_weight=<span class="keyword">None</span>, init_score=<span class="keyword">None</span>, eval_set=<span class="keyword">None</span>, eval_names=<span class="keyword">None</span>, eval_sample_weight=<span class="keyword">None</span>, eval_init_score=<span class="keyword">None</span>, eval_metric=<span class="string">'logloss'</span>, early_stopping_rounds=<span class="keyword">None</span>, verbose=<span class="keyword">True</span>, feature_name=<span class="string">'auto'</span>, categorical_feature=<span class="string">'auto'</span>, callbacks=<span class="keyword">None</span>)`</span><br><span class="line"><span class="comment"># sample_weight：每个训练样本的权重</span></span><br><span class="line"><span class="comment"># init_score：每个训练样本的初始分数</span></span><br><span class="line"><span class="comment"># group：每个训练样本的分组</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># eval_set：设置验证集</span></span><br><span class="line"><span class="comment"># eval_names：设置验证集的名字</span></span><br><span class="line"><span class="comment"># eval_sample_weight：验证集中每个样本的权重</span></span><br><span class="line"><span class="comment"># eval_init_score：每个验证集中，每个样本的init score</span></span><br><span class="line"><span class="comment"># eval_group：每个验证集中，每个样本的分组</span></span><br><span class="line"><span class="comment"># eval_metric：验证的方法</span></span><br><span class="line"></span><br><span class="line">gbm.predict(data, num_iteration=<span class="number">-1</span>, raw_score=<span class="keyword">False</span>, pred_leaf=<span class="keyword">False</span>, pred_contrib=<span class="keyword">False</span>, data_has_header=<span class="keyword">False</span>, is_reshape=<span class="keyword">True</span>, pred_parameter=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment"># num_iteration`：设置在预测时，使用多少个子树</span></span><br><span class="line"><span class="comment"># raw_score`：是否输出raw score</span></span><br><span class="line"><span class="comment"># pred_leaf： 输出每个样本在每个子树的哪个叶子上</span></span><br><span class="line"><span class="comment"># pred_contrib：输出每个特征对每个样本预测结果的贡献程度</span></span><br><span class="line"><span class="comment"># data_has_header：指示数据集是否含有标题</span></span><br><span class="line"><span class="comment"># is_reshape：是否reshape</span></span><br><span class="line"><span class="comment"># pred_parameter：给出其它的参数</span></span><br><span class="line"></span><br><span class="line">gbm.predict_proba(X, raw_score=<span class="keyword">False</span>, num_iteration=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>LGBMRegressor 是 LGBMModel 的子类，它用于回归任务</li>
<li>LGBMRanker 是 LGBMModel 的子类，它用于排序任务，详见排序学习算法</li>
</ul>
<h3 id="绘图-API"><a href="#绘图-API" class="headerlink" title="绘图 API"></a>绘图 API</h3><h3 id="booster-API"><a href="#booster-API" class="headerlink" title="booster API"></a>booster API</h3><h3 id="LightGBM-示例"><a href="#LightGBM-示例" class="headerlink" title="LightGBM 示例"></a>LightGBM 示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV  <span class="comment"># Perforing grid search</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(<span class="string">'train.csv'</span>)   <span class="comment"># 读取数据</span></span><br><span class="line">y = train_data.pop(<span class="string">'30'</span>).values   <span class="comment"># 用pop方式将训练数据中的标签值y取出来，作为训练目标，这里的‘30’是标签的列名</span></span><br><span class="line">col = train_data.columns</span><br><span class="line">x = train_data[col].values  <span class="comment"># 剩下的列作为训练数据</span></span><br><span class="line">train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=<span class="number">0.333</span>, random_state=<span class="number">0</span>)   <span class="comment"># 分训练集和验证集</span></span><br><span class="line">train = lgb.Dataset(train_x, train_y)</span><br><span class="line">valid = lgb.Dataset(valid_x, valid_y, reference=train)</span><br><span class="line"></span><br><span class="line">parameters = &#123;</span><br><span class="line">              <span class="string">'max_depth'</span>: [<span class="number">15</span>, <span class="number">20</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>],</span><br><span class="line">              <span class="string">'learning_rate'</span>: [<span class="number">0.01</span>, <span class="number">0.02</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.15</span>],</span><br><span class="line">              <span class="string">'feature_fraction'</span>: [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">0.95</span>],</span><br><span class="line">              <span class="string">'bagging_fraction'</span>: [<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">0.95</span>],</span><br><span class="line">              <span class="string">'bagging_freq'</span>: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>],</span><br><span class="line">              <span class="string">'reg_alpha'</span>: [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.6</span>],</span><br><span class="line">              <span class="string">'reg_lambda'</span>: [<span class="number">0</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">35</span>, <span class="number">40</span>],</span><br><span class="line">              <span class="string">'cat_smooth'</span>: [<span class="number">1</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">35</span>]</span><br><span class="line">&#125;</span><br><span class="line">gbm = lgb.LGBMClassifier(boosting_type=<span class="string">'gbdt'</span>,</span><br><span class="line">                         objective = <span class="string">'binary'</span>,</span><br><span class="line">                         metric = <span class="string">'auc'</span>,</span><br><span class="line">                         verbose = <span class="number">0</span>,</span><br><span class="line">                         learning_rate = <span class="number">0.01</span>,</span><br><span class="line">                         num_leaves = <span class="number">35</span>,</span><br><span class="line">                         feature_fraction=<span class="number">0.8</span>,</span><br><span class="line">                         bagging_fraction= <span class="number">0.9</span>,</span><br><span class="line">                         bagging_freq= <span class="number">8</span>,</span><br><span class="line">                         reg_alpha= <span class="number">0.6</span>,</span><br><span class="line">                         reg_lambda= <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">gsearch = GridSearchCV(gbm, param_grid=parameters, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">3</span>)</span><br><span class="line">gsearch.fit(train_x, train_y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Best score: %0.3f"</span> % gsearch.best_score_)</span><br><span class="line">print(<span class="string">"Best parameters set:"</span>)</span><br><span class="line">best_parameters = gsearch.best_estimator_.get_params()</span><br><span class="line"><span class="keyword">for</span> param_name <span class="keyword">in</span> sorted(parameters.keys()):</span><br><span class="line">    print(<span class="string">"\t%s: %r"</span> % (param_name, best_parameters[param_name]))</span><br></pre></td></tr></table></figure>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>KevinW</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2019/08/01/boosting/">https://hyqskevin.github.io/2019/08/01/boosting/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</li></ul></div><br><div class="tags"><a href="../../../../tags/boosting/">boosting</a></div><div class="post-nav"><a class="pre" href="../../04/write_log/">如何写日志</a><a class="next" href="../../../07/30/scikit-learn-note/">scikit-learn 文档学习笔记(1)</a></div></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 学习分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/code/">code</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/language-learning/">language-learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/note/">note</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/notes/">notes</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/paper/">paper</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/repo/">repo</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/study/">study</a><span class="category-list-count">26</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="../../../../tags/others/" style="font-size: 15px;">others</a> <a href="../../../../tags/MTG/" style="font-size: 15px;">MTG</a> <a href="../../../../tags/algorithm/" style="font-size: 15px;">algorithm</a> <a href="../../../../tags/Vue/" style="font-size: 15px;">Vue</a> <a href="../../../../tags/book/" style="font-size: 15px;">book</a> <a href="../../../../tags/boosting/" style="font-size: 15px;">boosting</a> <a href="../../../../tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="../../../../tags/c/" style="font-size: 15px;">c++</a> <a href="../../../../tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="../../../../tags/flask/" style="font-size: 15px;">flask</a> <a href="../../../../tags/time-series/" style="font-size: 15px;">time_series</a> <a href="../../../../tags/ensemble-learning/" style="font-size: 15px;">ensemble_learning</a> <a href="../../../../tags/bp-nn/" style="font-size: 15px;">bp_nn</a> <a href="../../../../tags/hexo/" style="font-size: 15px;">hexo</a> <a href="../../../../tags/JavaScript/" style="font-size: 15px;">JavaScript</a> <a href="../../../../tags/laravel/" style="font-size: 15px;">laravel</a> <a href="../../../../tags/python/" style="font-size: 15px;">python</a> <a href="../../../../tags/bootstrap2-0/" style="font-size: 15px;">bootstrap2.0</a> <a href="../../../../tags/php/" style="font-size: 15px;">php</a> <a href="../../../../tags/machine-learning/" style="font-size: 15px;">machine_learning</a> <a href="../../../../tags/mysql/" style="font-size: 15px;">mysql</a> <a href="../../../../tags/pandas/" style="font-size: 15px;">pandas</a> <a href="../../../../tags/slam/" style="font-size: 15px;">slam</a> <a href="../../../../tags/session/" style="font-size: 15px;">session</a> <a href="../../../../tags/cookie/" style="font-size: 15px;">cookie</a> <a href="../../../../tags/Linux-device/" style="font-size: 15px;">Linux_device</a> <a href="../../../../tags/log/" style="font-size: 15px;">log</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="../../../../2020/11/26/EDH-combo/">Temer EDH combo</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2020/11/10/vue-notes/">Vue功能实现和使用技巧</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2020/05/28/better-scroll/">BetterScroll插件实现页面滚动效果</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2020/05/12/swiper/">swiper滑动插件</a></li><li class="post-list-item"><a class="post-list-link" href="../../../../2020/05/10/js同步和异步/">javascript异步，消息队列和事件循环</a></li><li class="post-list-item"><a class="post-list-link" href="../../../11/10/json-server/">json数据模拟</a></li><li class="post-list-item"><a class="post-list-link" href="../../20/imbalanced_data_analysis/">imblearn API</a></li><li class="post-list-item"><a class="post-list-link" href="../../10/decision-tree-visualization/">tip:决策树、随机森林结果可视化</a></li><li class="post-list-item"><a class="post-list-link" href="../../04/write_log/">如何写日志</a></li><li class="post-list-item"><a class="post-list-link" href>boosting</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="../../../../." rel="nofollow">MonoShow.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="../../../../js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="../../../../js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/javascript" src="../../../../js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" color="0,0,0" opacity="0.5" zindex="-2" count="50" src="//lib.baomitu.com/canvas-nest.js/2.0.3/canvas-nest.umd.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="../../../../js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="../../../../js/smartresize.js?v=0.0.0"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"left","width":100,"height":200},"mobile":{"show":true},"log":false});</script></body></html>